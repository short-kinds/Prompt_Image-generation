import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login


def llama3(news_summary):
    # 허가받은 HF Access Token 입력
    token = ""
    if token:
        try:
            login(token)
        except Exception:
            pass

    model_id = "meta-llama/Llama-3.1-8B-Instruct"

    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, token=token)

    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto"             # 여러 GPU면 자동 분산
    )

    system_msg = (
            "You are an expert prompt writer for image generation models. "
            "Write a single, concise English prompt optimized for GPT-Image 1 "
            "to generate a 1024x1024 cartoon-style illustration based on the user's news summary. "
            "Embed all constraints directly in the prompt. "
            "Return ONLY the prompt text, no explanations."
        )

    user_msg = (
            f"News summary:\n{news_summary}\n\n"
            "Requirements for the prompt you will write:\n"
            "- Cartoon style, 1024x1024.\n"
            "- Simple, clean background.\n"
            "- No text, no logos, no watermarks.\n"
            "- No blur; crisp details.\n"
            "- No extra fingers; realistic hand anatomy.\n"
            "- Cohesive lighting and color harmony.\n"
            "- Do not mention camera brands or technical metadata.\n"
            "Output only the final prompt."
        )

    messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg}
        ]

    chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    inputs = tokenizer(chat, return_tensors="pt").to(model.device)

    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=300,         # 모델이 새로 생성할 최대 토큰 수
            do_sample=True,             # False --> 모델이 가장 높은 확률의 단어만 고름 (프롬프트가 반복적/뻣뻣함), True --> 확률적 샘플링으로 매번 다른 창의적인 결과 도출
            temperature=0.7,            # 1.0 = 원본 확률 그대로, <1.0 = 높은 확률에 더 큰 가중치 (뒷 말에 뭐가 올지 각 단어 확률에 대해서)
            top_p=0.9,                  # 출력 후보 중 확률 누적 합이 p가 될때의 상위 토큰만 남기고 나머진 버림
            repetition_penalty=1.05     # 모델이 같은 단어를 반복하는 걸 막기 위한 패널티, 높을 수록 패널티 강함
        )

    # 신규 생성 부분만 디코딩
    new_tokens = out[0, inputs["input_ids"].shape[-1]:]      # 0~입력 프롬프트는 버리고 새로 생성한 프롬프트만 추출
    text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()    # 특수 토큰 및 공백 제거

    # 간단한 후처리: 따옴표/코드블럭 제거 가능
    text = text.strip().strip('"').strip("'").strip()       # ""나 '', ... 와 같은 포맷 제거

    print("프롬프트 생성 완료!!")
    print("prompt: ", text)

    return text
